---
title: "Statistical Modelling with R"
message: FALSE
warning: FALSE
---

```{r, echo = FALSE}
library(knitr)
library(grid)
```


```{r, echo = FALSE}
cl = function(x, color = "blue"){
  outputFormat = knitr::opts_knit$get("rmarkdown.pandoc.to")
  if(outputFormat == 'latex')
    paste("\\textcolor{",color,"}{", x ,"}",sep="")
  else if(outputFormat == 'html')
    paste("<font color='",color,"'>", x ,"</font>",sep="")
  else
    x
}
```

```{r, echo=FALSE, message = FALSE, warning = FALSE, results='hide'}
library(tidyverse)
library(lsmeans)
library(broom)
library(modelr)
library(knitr)
library(grid)
```


One has a data set with some **variables** of interest (length, weight, catch etc.) How do we go about analyzing this?


# Step 1: **Descriptive statistics** 

------------------------------------------------------------------------

We start off with data exploration, visualization and summarize data in a meaningful way. This is something we have covered in the visualization and transformation modules above.

**Spatial maps** are appealing provided one has spatial data (e.g. latitude and longitude values of stations).

Some useful pointers when exploring your data set:

- For **Discrete variable** - Pie charts / Bar Graphs (usually better). All categorical variables are discrete and some numerical variables e.g., numbers of fish. One counts discrete data (e.g. 20 fish).

- For **Continuous variables** - histograms, scatterplots, boxplots are more appropriate. Continuous means when a numerical variable can have any numerical value on some intervals. One measures continuous data  (e.g., length is measured as 13.46cm).

## Distribution shapes

There are different shapes of distribution one should be aware of when plotting the data because this can affect the choice of statistic to be used. If the distribution is skewed, bimodal or multimodal the median shall be used rather then the mean. The median should also be preferred if there are outliers in the measurement. 


```{r echo=FALSE, message = FALSE}
knitr::include_graphics("images/distribution_shapes.png")
```

Also when distributions are skewed, data can be transformed to achieve normal distribution using $log$ or $sqrt$. 

## Outliers

Outliers are measurements that differ greatly from other measurements in the sample. Important to look at them specifically and consider their cause. They can be identified during the data exploration process. 

```{r echo=FALSE, message = FALSE}
include_graphics("images/outliers.png")
```

## Missing values

NAs were briefly introduced in the transformation module. NAs are not the same as zeros! It indicates missingness of data, meaning it was not measured. It is important to remove or replace NAs before calculating data summaries. 

## Statistics that describe central tendency and spread

Measures of central tendency were introduced in the transformation module i.e. **mean**, **median**. In addition, we would like to know the spread of the data.
These statistics describe how spread out the measurements are.

-   **Range**: obtained by $x_{max} - x_{min}$ 

-   **Quartile**: There are three quartiles

    ```{r, echo = FALSE}
    include_graphics("images/quartiles.png")
    ```

-   **Interquartile range**: $Q_3 - Q_1$

-   **Five number summary**: is informative for skewed distributions

    $$min, Q_1, Q_2, Q_3, max$$

```{r}
w <- 
  read_csv("ftp://ftp.hafro.is/pub/data/csv/minke.csv")

w %>% summary()

summary(w)

w %>% select(age) %>% summary()
```

-   **Variance** $s^2$: a measure of dispersion around the mean

-   **Standard deviation** $s$: square root of variance

-   **Standard error** - $se$: $s/sqrt(n)$


**Descriptive analysis describes the characteristics of a data set** and can summarize based on groups in the data but **conclusions are not drawn beyond the data we have**. 


# Step 2 - **Inferential Statistics**

------------------------------------------------------------------------

**Inferential statistics** are techniques that allow us to use the sample to generalize to the population or make inferences about the population. Sample should be an accurate representation of the population.


## Population and Sample

**Sample** - the set of subjects that are sampled from a given population (e.g., landed catch samples from landing sites)

**Population** - the set of all subjects that inference should be made about (e.g., skipjack tuna population in Saint Lucian waters)

Our data is almost always a **sample**


## Types of inferential statistics

------------------------------------------------------------------------

**Regression Analysis** and **hypothesis testing** are examples of inferential statistics. 


**Regression models** show the relationship between your dependent variable (response variable *Y*) and a set of independent variables (explanatory variable *X*). This statistical method lets you predict the value of the dependent variable based on different values of the independent variables. Hypothesis tests are incorporated in this procedure to determine whether the relationships observed in sample data actually exist in the data set.  

## The ideology behind hypothesis tests 

1.  A hypothesis is formed $H_1$ which describes what we want to demonstrate and another that describes a null $H_0$ case.

2.  A statistic is found that has a known probabiliy distribution. It is defined what values of the test statistics are "improbable" according to the probability distribution in the null case. 

3.  If the retrieved estimate classifies as "improbable" the hypothesis for the null case is rejected and the alternate hypothesis is claimed (use $p$-value to decide).

Those who would like to learn more about these concepts can use statistics resources available online:

* [Practical Regression and Anova using R](http://cran.hafro.is/doc/contrib/Faraway-PRA.pdf)
* [Tutor-Web](http://tutor-web.net)
* [Stats on StackExchange](http://stats.stackexchange.com/)
* [OpenIntroStatistics](https://www.openintro.org/stat/)


# Linear Models

------------------------------------------------------------------------

## Statistical Models

```{r echo = FALSE, message = FALSE}
include_graphics("images/model.png")
```

## **Purpose** 

-   Fit our model to our observed data
-   This fitting is an estimation procedure
-   To describe the relationship between $Y$ and $X$
-   To determine how much of the variation (uncertainty) in $Y$ can be explained by the relationship with $X$ and how much is unexplained (error)
-   To predict new values of $Y$ from new values of $X$
-   The modelling procedure can help us decide which models give the best fit to our observed sample data
-   Model diagnostics is important to ensure the model is adequate

## What does 'linear' mean? 

The term linear model is used in two distinct ways.

-   First, a model of a straight line relationship (most common)

-   Second, a model in which any variable of interest ($y$) is described by a linear combination of a series of parameters. The regression function is linear. Now linear refers to a combination of parameters not the shape of the relationship.

Using the second definition linear models can represent not only straight line relationships but also curvilinear relationships such as the models with polynomial terms.

## Linear Models

```{r echo = FALSE, message = FALSE}
include_graphics("images/linear_models.png")
```

## Simple linear model (Linear Regression)

Model formulation:

$$Y = \beta_0 + \beta_1X + \epsilon $$ where:

-   Y is the response variable
-   $\beta_0$ and $\beta_1$ are parameters to be estimated
-   $\epsilon$ is the error term that assumes normal distribution with mean 0 and standard deviation $\sigma^2$


When fitting statistical models the aim is to "tweak" model settings until they can "mimic" the data optimally. In the case of linear regression we want to estimate the best line through a cloud of points:
```{r, echo = FALSE}
dat <- data_frame(x=1:100/10,y=x + rnorm(100,sd=1)) 
dat %>% 
  ggplot(aes(x,y)) + geom_point()
```


The best line is the one that minimizes the difference between the data and the (model) predictions. Typically the model is of the form:
  $$ y = \alpha + \beta x + \ldots $$
  we want to choose parameters $\alpha$, $\beta$ and others such that this distance is minimized:
  
```{r,echo=FALSE}
dat %>% mutate(yhat = 2*x-5) %>% ggplot(aes(x,y)) + geom_point() + geom_line(aes(y=yhat)) + geom_segment(aes(xend=x,yend=yhat),col='blue') + annotate(x=1,y=10,label='a=-5,b=2',geom='label')
```

In addition to finding the optimal values of $\alpha$ and $\beta$ we want to know if these values are significantly different from zero. In mathematical terms we want to draw inferences (hypothesis testing) of the form:
  $$H_0 : \qquad \beta = 0 $$
  vs.
$$H_1 : \qquad \beta \neq 0 $$
  
# Linear models in R

------------------------------------------------------------------------
  
R provides a suite of tools for model building. Here we will use some simulated data to demonstrate model fitting. Linear regression models are created using the `lm` function:

```{r}
dat <- tibble(x=1:100/10, y= x + rnorm(100,sd=1)) 
fit <- lm(y~x, data=dat)
fit
```
where `y~x` is the formula to which parameter should be estimated, `y` is the dependent variable and `x` is explanatory variable and the `~` indicates that the variable `x` should be used to explain `y`. 

The `lm` function estimates the coefficients of the formula, and note that `lm` assumes that an intercept ($\alpha$) is estimated as well as the slope ($\beta$). The output from the `summary` function shows the model output and key statistics from a model. The model coefficients are tabulated along with standard error estimates and p-values. 

```{r}
summary(fit)
```


## Demystifying R model output

------------------------------------------------------------------------
  

```{r echo = FALSE}
include_graphics("images/demystify.png")
```

## Residuals

**Residuals** of the model showing median and interquartile range. How symmetrical are they? Can use boxplots to check.

```{r echo = FALSE}
include_graphics("images/R_SLRoutput_1.png")
```

## Model estimates {.smaller}

```{r echo = FALSE}
include_graphics("images/R_SLRoutput_2.png")
```

-   **Intercept** - represents the mean value of the response variable when all of the predictor variables in the model are equal to zero

-   **x (**$\beta_1$**)** - is the expected difference in y for 1 unit increase in x

-   The coefficient's **standard error** (St. Error) can be used to compute a confidence interval for the estimates.

## Significance Level 

```{r echo = FALSE}
include_graphics("images/R_SLRoutput_3.png")
```

-   **t-value** --  how many standard deviations the coefficient estimate is away from zero, want it to be far from 0!

-   **Pr (\>t)** --  probability of observing any value equal or higher than *t,* want small *p*, less than 5% is good.

-   The coefficient of the variable x is associated with a p-value \< 0.05, therefore, we can say that x has a statistically significant effect on y

## Goodness-of-fit 

```{r echo = FALSE}
include_graphics("images/R_SLRoutput_4.png")
```

-   **Residual standard error** - measures how well the regression line fits the data. The smaller the better (100 data points, 1 parameter gives 99 degrees of freedom).

-   **Multiple R-squared** is the proportion of variance in y explained by the predictors **Adjusted R-squared** - Adjusts for number of variables considered ($R^2$) (will always increase with more variables that are included)

-   **F-statistic** - Indicator of whether there is a relationship between predictor and response variables, the further away from 1 the better


## Compiling estimates in a tidy way

------------------------------------------------------------------------

To extract these numbers explicitly as a data frame the `broom` package provides a nice function called `tidy`:

```{r}
tidy(fit)
```
and the confidence intervals can be calculated as:

```{r}
tidy(fit,conf.int = TRUE)
```

And to plot the prediction and residuals the `modelr` package has two functions, `add_predictions` and `add_residuals`:
```{r,out.width = "50%",fig.show = "hold",warning=FALSE,message=FALSE}
aug.dat <- 
  dat %>% 
  add_predictions(fit) %>% 
  add_residuals(fit) 
## plot predictions
aug.dat %>%   
  ggplot(aes(x,y)) + geom_point() + geom_line(aes(y=pred),col='blue')
## plot residuals
aug.dat %>% 
  ggplot(aes(resid)) + geom_histogram()
```

## Model Diagnostics

------------------------------------------------------------------------

For the model to be adequate, two main assumptions should hold:

 - the error from the model should be normally distributed
 - the homogeneity of variance (homoscedasticity) i.e the variance is equal and the error is constant along the values of the dependent variable


```{r}
plot(fit)
```

The diagnostic plots show residuals in four different ways:

- Residuals vs Fitted. Used to check the linear relationship assumptions. A horizontal line, without distinct patterns is an indication for a linear relationship (the red line should be more or less straight around 0).

- Normal Q-Q plot. Used to examine whether the residuals are normally distributed. It’s good if residuals points follow the straight dashed line. Some deviations along the edges are acceptable.

- Scale-Location (or Spread-Location). Used to check the homogeneity of variance of the residuals (homoscedasticity). Horizontal line with equally spread points is a good indication of homoscedasticity. 

- Residuals vs Leverage. Used to identify influential cases, that is extreme values that might influence the regression results when included or excluded from the analysis. Will show outliers that may influence model fitting.

# Fitting length and weight of cod

------------------------------------------------------------------------

Now as a real example consider the length--weight relationship for cod.

```{r}
download.file("https://www.hafro.is/~einarhj/older/data/nsibts_tidy.rda",
            destfile = "data/nsibts_tidy.rda")

load("data/nsibts_tidy.rda")

d <- 
  tidy_ag %>% 
  filter(latin == 'Gadus morhua', !is.na(wgt),!is.na(age)) %>% 
  left_join(tidy_st) %>% 
  filter(!is.na(depth))

ggplot(d,aes(length,wgt,col=sex)) + geom_point()
```

we see that the relationship between length and weight is probably not linear but as first approximation try that:
```{r}
fit <- lm(wgt~length,data=d)
fit
```
hmm fish a 0 cm is -2.4 kg:) That is not really plausible. But look at the usual summaries:
```{r}
summary(fit)
```
Everything is highly significant, as you would expect with this wealth of data, but looking at the fit to the data and residual:
  
```{r, echo=FALSE,out.width = "50%",fig.show = "hold",warning=FALSE,message=FALSE}
aug.dat <- 
  d %>% 
  add_predictions(fit) %>% 
  add_residuals(fit) 
## plot predictions
aug.dat %>%   
  ggplot(aes(length,wgt)) + geom_point() + geom_line(aes(y=pred),col='blue')
## plot residuals
aug.dat %>% 
  ggplot(aes(resid)) + geom_histogram()
```

we see immediately that the model does not perform well on the tails and the residual error is heavily skewed. So let's do something more realistic and log transform the data:
```{r}
fit <- lm(log(wgt)~log(length),data=d)
fit
```

This looks more sensible. Now lets plot the results:
```{r}
aug.dat <- 
  d %>% 
  add_predictions(fit) %>% 
  add_residuals(fit) 
## plot predictions, note we need transform predictions back to non-log space
aug.dat %>%   
  ggplot(aes(length,wgt)) + geom_point() + geom_line(aes(y=exp(pred)),col='blue')
## plot residuals
aug.dat %>% 
  ggplot(aes(resid)) + geom_histogram()
```

We can then add variable to the regression by adding to the formula:
```{r}
fit2 <- lm(log(wgt)~log(length)+sex,data=d)
summary(fit2)
```

and we can also add interaction terms, where the `log10(length)*sex` is a shorthand notation for `log10(length) + sex + log10(length):sex`:
```{r}
fit3 <- lm(log(wgt)~log(length)*sex,data=d)
summary(fit3)
```

# Generalized linear model

------------------------------------------------------------------------

## Model formulation and output 

Generalized Linear models in R are formulated using the **glm** function

```{r echo = FALSE}
include_graphics("images/glm_output.png")
```

- Reports deviance instead of variance

Note there is no $R^2$ reported but deviance explained can be obtained using 

`1 - residual deviance/null deviance)`


## Analysis of Variance (ANOVA) 

ANOVA is essentially a test for the equality of means between two (or more) groups. ANOVA output is slightly different from linear model output because groups means are being tested. In R one uses  `aov` function to test this class of hypotheses of whether groups means are different:

Model formulation:

    aov(formula = length ~ as.factor(year), data = dat)

```{r echo = FALSE}
include_graphics("images/aov_output.png")
```

## One-way anova
As illustration of how one would perform an ANOVA in R consider the mean length caught by year:

```{r}
ggplot(d,aes(year,length,group=round(year))) + geom_boxplot()
```

```{r}
## note we need change the year to factor 
fit <- aov(length~as.factor(year),data=d)
fit
```
To get the results from the ANOVA one typically needs to use the `summary` function:
```{r}
summary(fit)
```
where we see the mean length is significantly different by year. And as above we can use `tidy`
```{r}
fit %>% tidy()
```

Now this is all well and good, but we now need to know which of these years are significantly different. This can be done using Tukey test, implemented using the `lsmeans` function:
```{r}
fit %>% lsmeans(pairwise~year)
```
But getting the results to a data.frame is bit more involved:
```{r}
ls.fit <- fit %>% lsmeans(pairwise~year)
ls.cont <- ls.fit$contrasts %>% summary() %>% as_data_frame()
ls.cont
```
And to find the years:
```{r}
ls.cont %>% filter(p.value < 0.05)
```

::: callout-tip
## Exercise

Test if weight is significantly different by year

:::